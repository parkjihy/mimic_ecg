# -*- coding: utf-8 -*-
"""250511

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeDnJa4wdW1LuYYGUcJFXXv4XX7l4DBn

# Download dataset
"""

# Google Drive ì—°ê²°
from google.colab import drive
drive.mount('/content/drive')

# ì €ì¥ ê²½ë¡œ ì§€ì • (Google Drive ë‚´ í´ë”)
save_path = '/content/drive/MyDrive/mimic_ecg/p1020'

# í´ë” ìƒì„±
!mkdir -p "{save_path}"

# PhysioNetì—ì„œ p1020 í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ
!wget -r -N -c -np -nH --cut-dirs=5 -P "{save_path}" https://physionet.org/files/mimic-iv-ecg/1.0/files/p1020/

# ì €ì¥ ê²½ë¡œ ì§€ì • (Google Drive ë‚´ í´ë”)
save_path = '/content/drive/MyDrive/mimic_ecg/p1015'

# í´ë” ìƒì„±
!mkdir -p "{save_path}"

# PhysioNetì—ì„œ p1020 í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ
!wget -r -N -c -np -nH --cut-dirs=5 -P "{save_path}" https://physionet.org/files/mimic-iv-ecg/1.0/files/p1015/

"""# labeling"""

# Google Drive ì—°ê²°
from google.colab import drive
drive.mount('/content/drive')

!pip install wfdb

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/AI/machine_measurements (1).csv", low_memory=False)
print(df.columns)

#ã…£
import os
import wfdb
import numpy as np

DATA_ROOT = "/content/drive/MyDrive/mimic_ecg"

def convert_npy_from_p1024(data_root, min_folder='p1000'):
    min_index = int(min_folder[1:])  # e.g. p1024 â†’ 1024

    for root, _, files in os.walk(data_root):
        folder_name = os.path.basename(os.path.dirname(root))
        try:
            folder_index = int(folder_name[1:])
        except:
            continue

        if folder_index < min_index:
            continue

        for file in files:
            if file.endswith('.hea'):
                record_path = os.path.join(root, file[:-4])  # without .hea
                try:
                    record = wfdb.rdrecord(record_path)
                    np.save(record_path + ".npy", record.p_signal)
                    os.remove(record_path + ".hea")
                    os.remove(record_path + ".dat")
                    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {record_path}.npy")
                except Exception as e:
                    print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {record_path} | ì´ìœ : {e}")

# ì‹¤í–‰
convert_npy_from_p1024(DATA_ROOT)

!pip install wfdb

#í•™ìŠµí•  .npy ë°ì´í„° ëª¨ì•„ë†“ê¸°
import os
import shutil

def extract_npy_files(src_dir, dest_dir):
    # ìƒˆ í´ë”ê°€ ì—†ë‹¤ë©´ ìƒì„±
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)

    # src_dirì—ì„œ .npy íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk(src_dir):
        for file in files:
            if file.endswith(".npy"):
                # .npy íŒŒì¼ ê²½ë¡œ
                src_file = os.path.join(root, file)
                dest_file = os.path.join(dest_dir, file)

                # .npy íŒŒì¼ì„ ìƒˆ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
                shutil.copy(src_file, dest_file)
                print(f"íŒŒì¼ ë³µì‚¬: {src_file} -> {dest_file}")

# ì‚¬ìš© ì˜ˆì‹œ
src_directory = "/content/drive/MyDrive/mimic_ecg"  # ì›ë³¸ .npy íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”
dest_directory = "/content/drive/MyDrive/mimic_ecg_filtered"  # .npy íŒŒì¼ì„ ì €ì¥í•  ìƒˆë¡œìš´ í´ë”

extract_npy_files(src_directory, dest_directory)

#ë¼ë²¨ë§
import pandas as pd

# CSV ë¡œë“œ
df = pd.read_csv("/content/drive/MyDrive/AI/machine_measurements (1).csv")

# ì „ì²˜ë¦¬
df["ecg_id"] = df["study_id"].astype(str)
df["report_0"] = df["report_0"].fillna("").str.lower()

# ì „ì²´ ìƒ˜í”Œ ìˆ˜
total = len(df)

# AFib ë¼ë²¨ë§
afib_df = df.copy()
afib_df["label"] = afib_df["report_0"].apply(lambda x: 1 if "atrial fibrillation" in x else 0)
afib_df[["ecg_id", "label"]].to_csv("afib_labels.csv", index=False)
afib_pos = afib_df["label"].sum()
afib_neg = total - afib_pos

# AFL ë¼ë²¨ë§
aflt_df = df.copy()
aflt_df["label"] = aflt_df["report_0"].apply(lambda x: 1 if "atrial flutter" in x else 0)
aflt_df[["ecg_id", "label"]].to_csv("aflt_labels.csv", index=False)
aflt_pos = aflt_df["label"].sum()
aflt_neg = total - aflt_pos

# ì¶œë ¥
print("AFib ì–‘ì„± ìƒ˜í”Œ ìˆ˜:", afib_pos)
print("AFib ìŒì„± ìƒ˜í”Œ ìˆ˜:", afib_neg)
print("AFL ì–‘ì„± ìƒ˜í”Œ ìˆ˜:", aflt_pos)
print("AFL ìŒì„± ìƒ˜í”Œ ìˆ˜:", aflt_neg)

afib_df[["ecg_id", "report_0", "label"]].to_csv("/content/drive/MyDrive/AI/labels/afib_labeled_full.csv", index=False)

print("0 ë¼ë²¨ íŒŒì¼ ìˆ˜:", sum(1 for v in label_dict.values() if v == 0))

import os

# AF, AFL ì™¸ ECGë“¤ë„ í¬í•¨ëœ ì „ì²´ ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]

print(f"[ì „ì²´ ECG íŒŒì¼ ìˆ˜] {len(all_files)}")

"""# Trainnig model"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils.class_weight import compute_class_weight

# ======================= âš™ï¸ ì„¤ì • ==========================
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # ë˜ëŠ” Atrial_Flutter.csv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# =================== íŒŒì¼ ë° ë¼ë²¨ ë¡œë“œ =========================
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))  # 0 or 1

all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
labels_for_split = [label_dict[f.split(".")[0]] for f in labeled_files]

print(f"[ì´ ECG íŒŒì¼ ìˆ˜] {len(all_files)}")
print(f"[ë¼ë²¨ í¬í•¨ íŒŒì¼ ìˆ˜] {len(labeled_files)}")
print(f"ì–‘ì„± ë¼ë²¨ ìˆ˜: {sum(labels_for_split)}")
print(f"ìŒì„± ë¼ë²¨ ìˆ˜: {len(labels_for_split) - sum(labels_for_split)}")

# ===================== Stratified Shuffle Split =====================
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in splitter.split(labeled_files, labels_for_split):
    train_files = [labeled_files[i] for i in train_idx]
    test_files = [labeled_files[i] for i in test_idx]

# ==================== Dataset í´ë˜ìŠ¤ =======================
class ECGDataset(Dataset):
    def __init__(self, file_list):
        self.file_list = file_list

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        filename = self.file_list[idx]
        filepath = os.path.join(data_dir, filename)
        signal = np.load(filepath)

        signal = np.nan_to_num(signal)
        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)

        signal_cnn = torch.tensor(signal.T, dtype=torch.float32)
        signal_lstm = torch.tensor(signal, dtype=torch.float32)
        label = torch.tensor(label_dict[filename.split(".")[0]], dtype=torch.float32)
        return signal_cnn, signal_lstm, label

# ==================== ëª¨ë¸ ì •ì˜ ==========================
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=12, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]
        return self.fc(out)

# ==================== í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ========================
def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("âŒ NaN loss ë°œìƒ")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x_cnn, x_lstm, labels in test_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = torch.sigmoid(model(inputs).squeeze())
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    preds_bin = [1 if p > 0.5 else 0 for p in all_preds]
    acc = accuracy_score(all_labels, preds_bin)
    f1 = f1_score(all_labels, preds_bin)
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except ValueError:
        auc = float('nan')
    print(f"âœ… Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")
    return {"acc": acc, "f1": f1, "auc": auc}

# =================== í•™ìŠµ ì‹œì‘ ============================
train_dataset = ECGDataset(train_files)
test_dataset = ECGDataset(test_files)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

models = [CNN(), LSTMModel()]
model_names = ["CNN", "LSTM"]
results = {}

for model, name in zip(models, model_names):
    print(f"\nğŸ“Œ [{name}] Training and Evaluation...")
    model_type = 'cnn' if name == "CNN" else 'lstm'
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics

print("ğŸ“Š ê²°ê³¼:", results)

# =================== ë¼ë²¨ ë¶„í¬ í™•ì¸ ========================
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
print("ğŸ” í•™ìŠµ ë¼ë²¨ ë¶„í¬:", dict(zip(*np.unique(train_labels, return_counts=True))))

test_labels = [label_dict[f.split(".")[0]] for f in test_files]
print("ğŸ” í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë¶„í¬:", dict(zip(*np.unique(test_labels, return_counts=True))))

#CNN_LSTM_SE ëª¨ë¸ êµ¬í˜„
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)

    def forward(self, x):
        # x: (batch, channels, seq_len)
        b, c, t = x.size()
        squeeze = x.mean(dim=2)  # (batch, channels)
        excitation = torch.relu(self.fc1(squeeze))
        excitation = torch.sigmoid(self.fc2(excitation)).unsqueeze(2)  # (batch, channels, 1)
        return x * excitation  # (batch, channels, seq_len)

class CNN_LSTM_SE(nn.Module):
    def __init__(self):
        super(CNN_LSTM_SE, self).__init__()
        self.conv1 = nn.Conv1d(12, 32, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.se1 = SEBlock(32)

        self.pool = nn.MaxPool1d(2)
        self.lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):  # x: (batch, 12, 5000)
        x = torch.relu(self.bn1(self.conv1(x)))  # (batch, 32, 5000)
        x = self.se1(x)  # (batch, 32, 5000)
        x = self.pool(x)  # (batch, 32, 2500)
        x = x.permute(0, 2, 1)  # (batch, 2500, 32)
        lstm_out, _ = self.lstm(x)  # (batch, 2500, 128)
        out = lstm_out[:, -1, :]  # (batch, 128)
        return self.fc(out)  # (batch, 1)

models = [CNN(), LSTMModel(), CNN_LSTM_SE()]
model_names = ["CNN", "LSTM", "CNN-LSTM-SE"]
results = {}

for model, name in zip(models, model_names):
    print(f"\nğŸ“Œ [{name}] Training and Evaluation...")
    model_type = 'cnn' if "CNN" in name else 'lstm'  # CNN, CNN-LSTM-SE â†’ cnn
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics

print("ğŸ“Š ê²°ê³¼:", results)

import matplotlib.pyplot as plt

def plot_results(results_dict):
    models = list(results_dict.keys())
    accs = [results_dict[m]["acc"] for m in models]
    f1s = [results_dict[m]["f1"] for m in models]
    aucs = [results_dict[m]["auc"] for m in models]

    x = range(len(models))
    width = 0.25

    plt.figure(figsize=(10, 6))
    plt.bar([i - width for i in x], accs, width=width, label='Accuracy')
    plt.bar(x, f1s, width=width, label='F1 Score')
    plt.bar([i + width for i in x], aucs, width=width, label='AUC')

    plt.xticks(ticks=x, labels=models)
    plt.ylabel("Score")
    plt.title("âœ… Model Comparison: Accuracy / F1 / AUC")
    plt.legend()
    plt.grid(axis='y')
    plt.ylim(0, 1.1)
    plt.show()



plot_results(results)

import matplotlib.pyplot as plt

def plot_results(results_dict):
    models = list(results_dict.keys())
    accs = [results_dict[m]["acc"] for m in models]
    f1s = [results_dict[m]["f1"] for m in models]
    aucs = [results_dict[m]["auc"] for m in models]

    plt.figure(figsize=(10, 6))

    # Plot lines with enhanced design
    plt.plot(models, accs, marker='o', label='Accuracy', linestyle='-', color='#1f77b4', markersize=8, linewidth=2)
    plt.plot(models, f1s, marker='s', label='F1 Score', linestyle='--', color='#ff7f0e', markersize=8, linewidth=2)
    plt.plot(models, aucs, marker='^', label='AUC', linestyle='-.', color='#2ca02c', markersize=8, linewidth=2)

    # Title and labels with professional fonts
    plt.title("âœ… Model Comparison: Accuracy / F1 / AUC", fontsize=16, fontweight='bold', family='Arial')
    plt.ylabel("Score", fontsize=14, fontweight='bold', family='Arial')
    plt.xticks(fontsize=12, family='Arial', rotation=45)
    plt.yticks(fontsize=12, family='Arial')

    # Adding gridlines and customizing background
    plt.grid(True, linestyle=':', color='gray', alpha=0.7)
    plt.gca().set_facecolor('#f9f9f9')  # Light background color

    # Adjust plot limits
    plt.ylim(0, 1.1)

    # Add legend with a refined style
    plt.legend(loc='upper left', fontsize=12, frameon=False)

    # Show the plot
    plt.tight_layout()  # Adjust layout for better fitting
    plt.show()

# Example usage:
# plot_results(results)

torch.save(model.state_dict(), f"{name}_best_model.pt")
print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {name}_best_model.pt")

#ëª¨ë¸ ì €ì¥
for model, name in zip(models, model_names):
    model_type = 'cnn' if "CNN" in name else 'lstm'
    print(f"\nğŸ“Œ [{name}] Training and Evaluation...")
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics
        torch.save(model.state_dict(), f"{name}_best_model.pt")
        print(f"ğŸ’¾ ì €ì¥ë¨: {name}_best_model.pt")

import torch
import numpy as np
import os
import pandas as pd

# Updated infer_and_save function
def infer_and_save(model, data_dir, test_files, model_type='cnn', threshold=0.5, save_path='output_predictions.csv'):
    model.eval()
    all_preds = []
    all_probs = []
    all_ecg_ids = []

    with torch.no_grad():
        for filename in test_files:
            # Get file path
            filepath = os.path.join(data_dir, filename)

            # Load the ECG signal from the .npy file
            signal = np.load(filepath)

            # Preprocess the signal (standardization)
            signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)
            signal_cnn = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0)  # (1, 5000, 12)
            signal_cnn = signal_cnn.permute(0, 2, 1)  # (1, 12, 5000) â†’ OK

            # Convert the signal to tensor for both CNN and LSTM
            signal_cnn = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0)  # Add batch dimension
            signal_lstm = torch.tensor(signal, dtype=torch.float32).unsqueeze(0)  # Add batch dimension

            # For CNN model, the shape should be (batch_size, channels, time_steps)
            if model_type == 'cnn':
                signal_cnn = signal_cnn.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)
                inputs = signal_cnn
            else:
                inputs = signal_lstm

            inputs = inputs.to(device)

            # Get the output probabilities from the model
            outputs = model(inputs)  # Output shape: (batch_size, 1) or (batch_size,)

            # Debugging print statements
            print(f"Inputs shape: {inputs.shape}")
            print(f"Model outputs shape: {outputs.shape}")

            # If the model output is (batch_size, 1), we squeeze to remove the singleton dimension
            if len(outputs.shape) > 1:
                outputs = outputs.squeeze()  # Remove extra dimension

            # Debugging print after squeezing
            print(f"Squeezed outputs shape: {outputs.shape}")

            # Ensure outputs is a scalar value
            if outputs.dim() == 0:
                outputs = outputs.item()  # Convert tensor to scalar

            # Debugging print for outputs
            print(f"Predicted output (probability): {outputs}")

            # Create binary predictions using threshold
            preds = (outputs > threshold).astype(int)

            # Collect results
            all_ecg_ids.append(filename.split('.')[0])
            all_probs.append(outputs)  # Single probability value
            all_preds.append(preds)  # Single prediction value

    # Create DataFrame and save it as a CSV file
    result_df = pd.DataFrame({
        'ecg_id': all_ecg_ids,
        'PROB': all_probs,
        'PRED': all_preds
    })

    # Save the results to a CSV file
    result_df.to_csv(save_path, index=False)
    print(f"âœ… Inference results saved to {save_path}")

# Load the trained models (CNN, LSTM, CNN-LSTM-SE)
cnn_model = CNN().to(device)
lstm_model = LSTMModel().to(device)
cnn_lstm_se_model = CNN_LSTM_SE().to(device)


# Load your trained models' weights (adjust path as needed)
cnn_model.load_state_dict(torch.load('/content/CNN_best_model.pt'))
lstm_model.load_state_dict(torch.load('/content/LSTM_best_model.pt'))
cnn_lstm_se_model.load_state_dict(torch.load('/content/CNN-LSTM-SE_best_model.pt'))

# Specify the directory where your evaluation ECG data is located
data_dir = '/content/drive/MyDrive/mimic_ecg_filtered/1027'  # Adjust this to your directory path

# List all .npy files in the evaluation dataset folder
test_files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]

# Inference on the test data using each model and save the results
infer_and_save(cnn_model, data_dir, test_files, model_type='cnn', save_path='cnn_predictions.csv')
infer_and_save(lstm_model, data_dir, test_files, model_type='lstm', save_path='lstm_predictions.csv')
infer_and_save(cnn_lstm_se_model, data_dir, test_files, model_type='cnn_lstm_se', save_path='cnn_lstm_se_predictions.csv')

"""# Analysis"""

import os
import numpy as np
import pandas as pd

# ë°ì´í„° ê²½ë¡œ ë° ë¼ë²¨ íŒŒì¼
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # ë˜ëŠ” Atrial_Flutter.csv

# ë¼ë²¨ íŒŒì¼ ë¡œë“œ
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))

# ì „ì²´ ECG íŒŒì¼ ë¦¬ìŠ¤íŠ¸
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
print(f"ì „ì²´ ECG íŒŒì¼ ìˆ˜: {len(all_files)}")

# ë¼ë²¨ í¬í•¨ëœ íŒŒì¼ë§Œ í•„í„°ë§
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
print(f"ë¼ë²¨ í¬í•¨ íŒŒì¼ ìˆ˜: {len(labeled_files)}")

# ë¼ë²¨ ë¶„í¬ ë¶„ì„
labels = [label_dict[f.split('.')[0]] for f in labeled_files]
labels = np.array(labels)
positive_count = np.sum(labels == 1)
negative_count = np.sum(labels == 0)

print(f"ì–‘ì„± ìƒ˜í”Œ ìˆ˜: {positive_count}")
print(f"ìŒì„± ìƒ˜í”Œ ìˆ˜: {negative_count}")
print(f"ì–‘ì„± ë¹„ìœ¨: {positive_count / len(labels):.2%}")
print(f"ìŒì„± ë¹„ìœ¨: {negative_count / len(labels):.2%}")

# ë¼ë²¨ ë¶„í¬ í™•ì¸
unique, counts = np.unique(labels, return_counts=True)
label_distribution = dict(zip(unique, counts))
print("ë¼ë²¨ ë¶„í¬:", label_distribution)

import matplotlib.pyplot as plt

counts = [sum(labels_for_split), len(labels_for_split) - sum(labels_for_split)]
labels = ['Positive (AFib)', 'Negative (No AFib)']

plt.figure(figsize=(6,6))
plt.pie(counts, labels=labels, autopct='%1.1f%%', colors=['red', 'lightblue'])
plt.title('Class Distribution in Dataset')
plt.show()



"""# Training model_2"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils.class_weight import compute_class_weight

# ================= âš™ï¸ ì„¤ì • =================
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # ë˜ëŠ” Atrial_Flutter.csv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# ============== ë¼ë²¨ ë° íŒŒì¼ ë¡œë“œ ================
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))

all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
labels_for_split = [label_dict[f.split(".")[0]] for f in labeled_files]

print(f"[ì´ ECG íŒŒì¼ ìˆ˜] {len(all_files)}")
print(f"[ë¼ë²¨ í¬í•¨ íŒŒì¼ ìˆ˜] {len(labeled_files)}")
print(f"ì–‘ì„± ë¼ë²¨ ìˆ˜: {sum(labels_for_split)}")
print(f"ìŒì„± ë¼ë²¨ ìˆ˜: {len(labels_for_split) - sum(labels_for_split)}")

# ============== Stratified Split ================
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in splitter.split(labeled_files, labels_for_split):
    train_files = [labeled_files[i] for i in train_idx]
    test_files = [labeled_files[i] for i in test_idx]

# ============== Dataset ì •ì˜ ===============
class ECGDataset(Dataset):
    def __init__(self, file_list):
        self.file_list = file_list

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        filename = self.file_list[idx]
        filepath = os.path.join(data_dir, filename)
        signal = np.load(filepath)
        signal = np.nan_to_num(signal)
        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)

        signal_cnn = torch.tensor(signal.T, dtype=torch.float32)
        signal_lstm = torch.tensor(signal, dtype=torch.float32)
        label = torch.tensor(label_dict[filename.split(".")[0]], dtype=torch.float32)
        return signal_cnn, signal_lstm, label

# ============== ëª¨ë¸ ì •ì˜ ===============
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        return self.fc2(x)

class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=12, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]
        return self.fc(out)

# ============== í›ˆë ¨ ë° í‰ê°€ ===============
def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("âŒ NaN loss ë°œìƒ")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x_cnn, x_lstm, labels in test_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = torch.sigmoid(model(inputs).squeeze())
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    threshold = 0.3
    preds_bin = [1 if p > threshold else 0 for p in all_preds]
    acc = accuracy_score(all_labels, preds_bin)
    f1 = f1_score(all_labels, preds_bin)
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except ValueError:
        auc = float('nan')
    print(f"âœ… Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")
    return {"acc": acc, "f1": f1, "auc": auc}

# ============== DataLoader ìƒì„± ===============
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
class_sample_count = np.bincount(train_labels)
weights = 1. / class_sample_count
samples_weight = [weights[label] for label in train_labels]
sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)

train_dataset = ECGDataset(train_files)
test_dataset = ECGDataset(test_files)

train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)
test_loader = DataLoader(test_dataset, batch_size=64)

# ============== ëª¨ë¸ í›ˆë ¨ ë° ê²°ê³¼ ì¶œë ¥ ===============
models = [CNN(), LSTMModel()]
model_names = ["CNN", "LSTM"]
results = {}

for model, name in zip(models, model_names):
    print(f"\nğŸ“Œ [{name}] Training and Evaluation...")
    model_type = 'cnn' if name == "CNN" else 'lstm'
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics
        torch.save(model.state_dict(), f"{name}_best_model.pt")
        print(f"ğŸ’¾ ì €ì¥ë¨: {name}_best_model.pt")

# ============== ë¼ë²¨ ë¶„í¬ ì¶œë ¥ ===============
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
print("ğŸ” í•™ìŠµ ë¼ë²¨ ë¶„í¬:", dict(zip(*np.unique(train_labels, return_counts=True))))

test_labels = [label_dict[f.split(".")[0]] for f in test_files]
print("ğŸ” í…ŒìŠ¤íŠ¸ ë¼ë²¨ ë¶„í¬:", dict(zip(*np.unique(test_labels, return_counts=True))))

# ============== ìµœì¢… ê²°ê³¼ ===============
print("\nğŸ“Š ìµœì¢… ê²°ê³¼:", results)

import torch
import numpy as np
import os
import csv
from tqdm import tqdm

# âš™ï¸ ì„¤ì •
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
model_path = "CNN_best_model.pt"
output_csv_path = "/content/drive/MyDrive/ECG_inference_result.csv"
threshold = 0.3  # ì›í•˜ëŠ” ì„ê³„ê°’
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ§  CNN ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# ğŸ“¥ ëª¨ë¸ ë¡œë“œ
model = CNN()
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

# ğŸ“‚ ì¶”ë¡  ëŒ€ìƒ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
evaluation_files = sorted(all_files)  # ëª¨ë“  ECG íŒŒì¼ ëŒ€ìƒìœ¼ë¡œ ì¶”ë¡  ìˆ˜í–‰

# ğŸ“¤ ê²°ê³¼ ì €ì¥
results = []

for filename in tqdm(evaluation_files, desc="ğŸ” ì¶”ë¡  ì¤‘"):
    filepath = os.path.join(data_dir, filename)
    signal = np.load(filepath)
    signal = np.nan_to_num(signal)
    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)
    signal_tensor = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(signal_tensor).squeeze()
        prob = torch.sigmoid(output).item()
        pred = 1 if prob > threshold else 0

    results.append({
        "ecg_id": filename.split(".")[0],
        "PROB": round(prob, 6),
        "PRED": pred
    })

# ğŸ’¾ CSVë¡œ ì €ì¥
with open(output_csv_path, "w", newline='') as f:
    writer = csv.DictWriter(f, fieldnames=["ecg_id", "PROB", "PRED"])
    writer.writeheader()
    writer.writerows(results)

print(f"âœ… ì¶”ë¡  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_csv_path}")

import matplotlib.pyplot as plt
import seaborn as sns

def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    loss_history = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("âŒ NaN loss ë°œìƒ")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(train_loader)
        loss_history.append(avg_loss)
        print(f"Epoch {epoch+1} | Loss: {avg_loss:.4f}")

    # í‰ê°€ ì½”ë“œëŠ” ìƒëµ

    return {"loss_history": loss_history}

# --- í›ˆë ¨ í›„ loss ì‹œê°í™” í•¨ìˆ˜ ---

def plot_loss_curve(loss_history, model_name):
    sns.set(style="whitegrid")
    plt.figure(figsize=(8,5))
    plt.plot(range(1, len(loss_history)+1), loss_history, marker='o', color='navy')
    plt.title(f'{model_name} Training Loss Over Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.xticks(range(1, len(loss_history)+1))
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

# --- ì˜ˆì‹œ ì‚¬ìš©ë²• ---
metrics = train_eval(CNN(), train_loader, test_loader, model_type='cnn', epochs=10)
if metrics:
    plot_loss_curve(metrics['loss_history'], "CNN")