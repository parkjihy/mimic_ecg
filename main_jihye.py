# -*- coding: utf-8 -*-
"""250511

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeDnJa4wdW1LuYYGUcJFXXv4XX7l4DBn

# Download dataset
"""

# Google Drive 연결
from google.colab import drive
drive.mount('/content/drive')

# 저장 경로 지정 (Google Drive 내 폴더)
save_path = '/content/drive/MyDrive/mimic_ecg/p1020'

# 폴더 생성
!mkdir -p "{save_path}"

# PhysioNet에서 p1020 폴더 내 모든 파일 다운로드
!wget -r -N -c -np -nH --cut-dirs=5 -P "{save_path}" https://physionet.org/files/mimic-iv-ecg/1.0/files/p1020/

# 저장 경로 지정 (Google Drive 내 폴더)
save_path = '/content/drive/MyDrive/mimic_ecg/p1015'

# 폴더 생성
!mkdir -p "{save_path}"

# PhysioNet에서 p1020 폴더 내 모든 파일 다운로드
!wget -r -N -c -np -nH --cut-dirs=5 -P "{save_path}" https://physionet.org/files/mimic-iv-ecg/1.0/files/p1015/

"""# labeling"""

# Google Drive 연결
from google.colab import drive
drive.mount('/content/drive')

!pip install wfdb

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/AI/machine_measurements (1).csv", low_memory=False)
print(df.columns)

#ㅣ
import os
import wfdb
import numpy as np

DATA_ROOT = "/content/drive/MyDrive/mimic_ecg"

def convert_npy_from_p1024(data_root, min_folder='p1000'):
    min_index = int(min_folder[1:])  # e.g. p1024 → 1024

    for root, _, files in os.walk(data_root):
        folder_name = os.path.basename(os.path.dirname(root))
        try:
            folder_index = int(folder_name[1:])
        except:
            continue

        if folder_index < min_index:
            continue

        for file in files:
            if file.endswith('.hea'):
                record_path = os.path.join(root, file[:-4])  # without .hea
                try:
                    record = wfdb.rdrecord(record_path)
                    np.save(record_path + ".npy", record.p_signal)
                    os.remove(record_path + ".hea")
                    os.remove(record_path + ".dat")
                    print(f"✅ 변환 완료: {record_path}.npy")
                except Exception as e:
                    print(f"❌ 변환 실패: {record_path} | 이유: {e}")

# 실행
convert_npy_from_p1024(DATA_ROOT)

!pip install wfdb

#학습할 .npy 데이터 모아놓기
import os
import shutil

def extract_npy_files(src_dir, dest_dir):
    # 새 폴더가 없다면 생성
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)

    # src_dir에서 .npy 파일 찾기
    for root, dirs, files in os.walk(src_dir):
        for file in files:
            if file.endswith(".npy"):
                # .npy 파일 경로
                src_file = os.path.join(root, file)
                dest_file = os.path.join(dest_dir, file)

                # .npy 파일을 새 디렉토리로 복사
                shutil.copy(src_file, dest_file)
                print(f"파일 복사: {src_file} -> {dest_file}")

# 사용 예시
src_directory = "/content/drive/MyDrive/mimic_ecg"  # 원본 .npy 파일들이 있는 폴더
dest_directory = "/content/drive/MyDrive/mimic_ecg_filtered"  # .npy 파일을 저장할 새로운 폴더

extract_npy_files(src_directory, dest_directory)

#라벨링
import pandas as pd

# CSV 로드
df = pd.read_csv("/content/drive/MyDrive/AI/machine_measurements (1).csv")

# 전처리
df["ecg_id"] = df["study_id"].astype(str)
df["report_0"] = df["report_0"].fillna("").str.lower()

# 전체 샘플 수
total = len(df)

# AFib 라벨링
afib_df = df.copy()
afib_df["label"] = afib_df["report_0"].apply(lambda x: 1 if "atrial fibrillation" in x else 0)
afib_df[["ecg_id", "label"]].to_csv("afib_labels.csv", index=False)
afib_pos = afib_df["label"].sum()
afib_neg = total - afib_pos

# AFL 라벨링
aflt_df = df.copy()
aflt_df["label"] = aflt_df["report_0"].apply(lambda x: 1 if "atrial flutter" in x else 0)
aflt_df[["ecg_id", "label"]].to_csv("aflt_labels.csv", index=False)
aflt_pos = aflt_df["label"].sum()
aflt_neg = total - aflt_pos

# 출력
print("AFib 양성 샘플 수:", afib_pos)
print("AFib 음성 샘플 수:", afib_neg)
print("AFL 양성 샘플 수:", aflt_pos)
print("AFL 음성 샘플 수:", aflt_neg)

afib_df[["ecg_id", "report_0", "label"]].to_csv("/content/drive/MyDrive/AI/labels/afib_labeled_full.csv", index=False)

print("0 라벨 파일 수:", sum(1 for v in label_dict.values() if v == 0))

import os

# AF, AFL 외 ECG들도 포함된 전체 데이터 디렉토리에서 파일 가져오기
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]

print(f"[전체 ECG 파일 수] {len(all_files)}")

"""# Trainnig model"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils.class_weight import compute_class_weight

# ======================= ⚙️ 설정 ==========================
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # 또는 Atrial_Flutter.csv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("✅ Using device:", device)

# =================== 파일 및 라벨 로드 =========================
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))  # 0 or 1

all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
labels_for_split = [label_dict[f.split(".")[0]] for f in labeled_files]

print(f"[총 ECG 파일 수] {len(all_files)}")
print(f"[라벨 포함 파일 수] {len(labeled_files)}")
print(f"양성 라벨 수: {sum(labels_for_split)}")
print(f"음성 라벨 수: {len(labels_for_split) - sum(labels_for_split)}")

# ===================== Stratified Shuffle Split =====================
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in splitter.split(labeled_files, labels_for_split):
    train_files = [labeled_files[i] for i in train_idx]
    test_files = [labeled_files[i] for i in test_idx]

# ==================== Dataset 클래스 =======================
class ECGDataset(Dataset):
    def __init__(self, file_list):
        self.file_list = file_list

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        filename = self.file_list[idx]
        filepath = os.path.join(data_dir, filename)
        signal = np.load(filepath)

        signal = np.nan_to_num(signal)
        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)

        signal_cnn = torch.tensor(signal.T, dtype=torch.float32)
        signal_lstm = torch.tensor(signal, dtype=torch.float32)
        label = torch.tensor(label_dict[filename.split(".")[0]], dtype=torch.float32)
        return signal_cnn, signal_lstm, label

# ==================== 모델 정의 ==========================
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=12, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]
        return self.fc(out)

# ==================== 학습 및 평가 함수 ========================
def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("❌ NaN loss 발생")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x_cnn, x_lstm, labels in test_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = torch.sigmoid(model(inputs).squeeze())
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    preds_bin = [1 if p > 0.5 else 0 for p in all_preds]
    acc = accuracy_score(all_labels, preds_bin)
    f1 = f1_score(all_labels, preds_bin)
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except ValueError:
        auc = float('nan')
    print(f"✅ Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")
    return {"acc": acc, "f1": f1, "auc": auc}

# =================== 학습 시작 ============================
train_dataset = ECGDataset(train_files)
test_dataset = ECGDataset(test_files)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

models = [CNN(), LSTMModel()]
model_names = ["CNN", "LSTM"]
results = {}

for model, name in zip(models, model_names):
    print(f"\n📌 [{name}] Training and Evaluation...")
    model_type = 'cnn' if name == "CNN" else 'lstm'
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics

print("📊 결과:", results)

# =================== 라벨 분포 확인 ========================
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
print("🔍 학습 라벨 분포:", dict(zip(*np.unique(train_labels, return_counts=True))))

test_labels = [label_dict[f.split(".")[0]] for f in test_files]
print("🔍 테스트 라벨 분포:", dict(zip(*np.unique(test_labels, return_counts=True))))

#CNN_LSTM_SE 모델 구현
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)

    def forward(self, x):
        # x: (batch, channels, seq_len)
        b, c, t = x.size()
        squeeze = x.mean(dim=2)  # (batch, channels)
        excitation = torch.relu(self.fc1(squeeze))
        excitation = torch.sigmoid(self.fc2(excitation)).unsqueeze(2)  # (batch, channels, 1)
        return x * excitation  # (batch, channels, seq_len)

class CNN_LSTM_SE(nn.Module):
    def __init__(self):
        super(CNN_LSTM_SE, self).__init__()
        self.conv1 = nn.Conv1d(12, 32, kernel_size=7, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.se1 = SEBlock(32)

        self.pool = nn.MaxPool1d(2)
        self.lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):  # x: (batch, 12, 5000)
        x = torch.relu(self.bn1(self.conv1(x)))  # (batch, 32, 5000)
        x = self.se1(x)  # (batch, 32, 5000)
        x = self.pool(x)  # (batch, 32, 2500)
        x = x.permute(0, 2, 1)  # (batch, 2500, 32)
        lstm_out, _ = self.lstm(x)  # (batch, 2500, 128)
        out = lstm_out[:, -1, :]  # (batch, 128)
        return self.fc(out)  # (batch, 1)

models = [CNN(), LSTMModel(), CNN_LSTM_SE()]
model_names = ["CNN", "LSTM", "CNN-LSTM-SE"]
results = {}

for model, name in zip(models, model_names):
    print(f"\n📌 [{name}] Training and Evaluation...")
    model_type = 'cnn' if "CNN" in name else 'lstm'  # CNN, CNN-LSTM-SE → cnn
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics

print("📊 결과:", results)

import matplotlib.pyplot as plt

def plot_results(results_dict):
    models = list(results_dict.keys())
    accs = [results_dict[m]["acc"] for m in models]
    f1s = [results_dict[m]["f1"] for m in models]
    aucs = [results_dict[m]["auc"] for m in models]

    x = range(len(models))
    width = 0.25

    plt.figure(figsize=(10, 6))
    plt.bar([i - width for i in x], accs, width=width, label='Accuracy')
    plt.bar(x, f1s, width=width, label='F1 Score')
    plt.bar([i + width for i in x], aucs, width=width, label='AUC')

    plt.xticks(ticks=x, labels=models)
    plt.ylabel("Score")
    plt.title("✅ Model Comparison: Accuracy / F1 / AUC")
    plt.legend()
    plt.grid(axis='y')
    plt.ylim(0, 1.1)
    plt.show()



plot_results(results)

import matplotlib.pyplot as plt

def plot_results(results_dict):
    models = list(results_dict.keys())
    accs = [results_dict[m]["acc"] for m in models]
    f1s = [results_dict[m]["f1"] for m in models]
    aucs = [results_dict[m]["auc"] for m in models]

    plt.figure(figsize=(10, 6))

    # Plot lines with enhanced design
    plt.plot(models, accs, marker='o', label='Accuracy', linestyle='-', color='#1f77b4', markersize=8, linewidth=2)
    plt.plot(models, f1s, marker='s', label='F1 Score', linestyle='--', color='#ff7f0e', markersize=8, linewidth=2)
    plt.plot(models, aucs, marker='^', label='AUC', linestyle='-.', color='#2ca02c', markersize=8, linewidth=2)

    # Title and labels with professional fonts
    plt.title("✅ Model Comparison: Accuracy / F1 / AUC", fontsize=16, fontweight='bold', family='Arial')
    plt.ylabel("Score", fontsize=14, fontweight='bold', family='Arial')
    plt.xticks(fontsize=12, family='Arial', rotation=45)
    plt.yticks(fontsize=12, family='Arial')

    # Adding gridlines and customizing background
    plt.grid(True, linestyle=':', color='gray', alpha=0.7)
    plt.gca().set_facecolor('#f9f9f9')  # Light background color

    # Adjust plot limits
    plt.ylim(0, 1.1)

    # Add legend with a refined style
    plt.legend(loc='upper left', fontsize=12, frameon=False)

    # Show the plot
    plt.tight_layout()  # Adjust layout for better fitting
    plt.show()

# Example usage:
# plot_results(results)

torch.save(model.state_dict(), f"{name}_best_model.pt")
print(f"💾 모델 저장 완료: {name}_best_model.pt")

#모델 저장
for model, name in zip(models, model_names):
    model_type = 'cnn' if "CNN" in name else 'lstm'
    print(f"\n📌 [{name}] Training and Evaluation...")
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics
        torch.save(model.state_dict(), f"{name}_best_model.pt")
        print(f"💾 저장됨: {name}_best_model.pt")

import torch
import numpy as np
import os
import pandas as pd

# Updated infer_and_save function
def infer_and_save(model, data_dir, test_files, model_type='cnn', threshold=0.5, save_path='output_predictions.csv'):
    model.eval()
    all_preds = []
    all_probs = []
    all_ecg_ids = []

    with torch.no_grad():
        for filename in test_files:
            # Get file path
            filepath = os.path.join(data_dir, filename)

            # Load the ECG signal from the .npy file
            signal = np.load(filepath)

            # Preprocess the signal (standardization)
            signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)
            signal_cnn = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0)  # (1, 5000, 12)
            signal_cnn = signal_cnn.permute(0, 2, 1)  # (1, 12, 5000) → OK

            # Convert the signal to tensor for both CNN and LSTM
            signal_cnn = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0)  # Add batch dimension
            signal_lstm = torch.tensor(signal, dtype=torch.float32).unsqueeze(0)  # Add batch dimension

            # For CNN model, the shape should be (batch_size, channels, time_steps)
            if model_type == 'cnn':
                signal_cnn = signal_cnn.permute(0, 2, 1)  # Convert to (batch_size, channels, time_steps)
                inputs = signal_cnn
            else:
                inputs = signal_lstm

            inputs = inputs.to(device)

            # Get the output probabilities from the model
            outputs = model(inputs)  # Output shape: (batch_size, 1) or (batch_size,)

            # Debugging print statements
            print(f"Inputs shape: {inputs.shape}")
            print(f"Model outputs shape: {outputs.shape}")

            # If the model output is (batch_size, 1), we squeeze to remove the singleton dimension
            if len(outputs.shape) > 1:
                outputs = outputs.squeeze()  # Remove extra dimension

            # Debugging print after squeezing
            print(f"Squeezed outputs shape: {outputs.shape}")

            # Ensure outputs is a scalar value
            if outputs.dim() == 0:
                outputs = outputs.item()  # Convert tensor to scalar

            # Debugging print for outputs
            print(f"Predicted output (probability): {outputs}")

            # Create binary predictions using threshold
            preds = (outputs > threshold).astype(int)

            # Collect results
            all_ecg_ids.append(filename.split('.')[0])
            all_probs.append(outputs)  # Single probability value
            all_preds.append(preds)  # Single prediction value

    # Create DataFrame and save it as a CSV file
    result_df = pd.DataFrame({
        'ecg_id': all_ecg_ids,
        'PROB': all_probs,
        'PRED': all_preds
    })

    # Save the results to a CSV file
    result_df.to_csv(save_path, index=False)
    print(f"✅ Inference results saved to {save_path}")

# Load the trained models (CNN, LSTM, CNN-LSTM-SE)
cnn_model = CNN().to(device)
lstm_model = LSTMModel().to(device)
cnn_lstm_se_model = CNN_LSTM_SE().to(device)


# Load your trained models' weights (adjust path as needed)
cnn_model.load_state_dict(torch.load('/content/CNN_best_model.pt'))
lstm_model.load_state_dict(torch.load('/content/LSTM_best_model.pt'))
cnn_lstm_se_model.load_state_dict(torch.load('/content/CNN-LSTM-SE_best_model.pt'))

# Specify the directory where your evaluation ECG data is located
data_dir = '/content/drive/MyDrive/mimic_ecg_filtered/1027'  # Adjust this to your directory path

# List all .npy files in the evaluation dataset folder
test_files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]

# Inference on the test data using each model and save the results
infer_and_save(cnn_model, data_dir, test_files, model_type='cnn', save_path='cnn_predictions.csv')
infer_and_save(lstm_model, data_dir, test_files, model_type='lstm', save_path='lstm_predictions.csv')
infer_and_save(cnn_lstm_se_model, data_dir, test_files, model_type='cnn_lstm_se', save_path='cnn_lstm_se_predictions.csv')

"""# Analysis"""

import os
import numpy as np
import pandas as pd

# 데이터 경로 및 라벨 파일
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # 또는 Atrial_Flutter.csv

# 라벨 파일 로드
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))

# 전체 ECG 파일 리스트
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
print(f"전체 ECG 파일 수: {len(all_files)}")

# 라벨 포함된 파일만 필터링
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
print(f"라벨 포함 파일 수: {len(labeled_files)}")

# 라벨 분포 분석
labels = [label_dict[f.split('.')[0]] for f in labeled_files]
labels = np.array(labels)
positive_count = np.sum(labels == 1)
negative_count = np.sum(labels == 0)

print(f"양성 샘플 수: {positive_count}")
print(f"음성 샘플 수: {negative_count}")
print(f"양성 비율: {positive_count / len(labels):.2%}")
print(f"음성 비율: {negative_count / len(labels):.2%}")

# 라벨 분포 확인
unique, counts = np.unique(labels, return_counts=True)
label_distribution = dict(zip(unique, counts))
print("라벨 분포:", label_distribution)

import matplotlib.pyplot as plt

counts = [sum(labels_for_split), len(labels_for_split) - sum(labels_for_split)]
labels = ['Positive (AFib)', 'Negative (No AFib)']

plt.figure(figsize=(6,6))
plt.pie(counts, labels=labels, autopct='%1.1f%%', colors=['red', 'lightblue'])
plt.title('Class Distribution in Dataset')
plt.show()



"""# Training model_2"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils.class_weight import compute_class_weight

# ================= ⚙️ 설정 =================
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
label_path = "/content/drive/MyDrive/AI/labels/Atrial_Fibrillation.csv"  # 또는 Atrial_Flutter.csv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("✅ Using device:", device)

# ============== 라벨 및 파일 로드 ================
label_df = pd.read_csv(label_path)
label_df["ecg_id"] = label_df["ecg_id"].astype(str)
label_dict = dict(zip(label_df["ecg_id"], label_df["PRED"]))

all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
labeled_files = [f for f in all_files if f.split(".")[0] in label_dict]
labels_for_split = [label_dict[f.split(".")[0]] for f in labeled_files]

print(f"[총 ECG 파일 수] {len(all_files)}")
print(f"[라벨 포함 파일 수] {len(labeled_files)}")
print(f"양성 라벨 수: {sum(labels_for_split)}")
print(f"음성 라벨 수: {len(labels_for_split) - sum(labels_for_split)}")

# ============== Stratified Split ================
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in splitter.split(labeled_files, labels_for_split):
    train_files = [labeled_files[i] for i in train_idx]
    test_files = [labeled_files[i] for i in test_idx]

# ============== Dataset 정의 ===============
class ECGDataset(Dataset):
    def __init__(self, file_list):
        self.file_list = file_list

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        filename = self.file_list[idx]
        filepath = os.path.join(data_dir, filename)
        signal = np.load(filepath)
        signal = np.nan_to_num(signal)
        signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)

        signal_cnn = torch.tensor(signal.T, dtype=torch.float32)
        signal_lstm = torch.tensor(signal, dtype=torch.float32)
        label = torch.tensor(label_dict[filename.split(".")[0]], dtype=torch.float32)
        return signal_cnn, signal_lstm, label

# ============== 모델 정의 ===============
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        return self.fc2(x)

class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=12, hidden_size=64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(64 * 2, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]
        return self.fc(out)

# ============== 훈련 및 평가 ===============
def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("❌ NaN loss 발생")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x_cnn, x_lstm, labels in test_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = torch.sigmoid(model(inputs).squeeze())
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    threshold = 0.3
    preds_bin = [1 if p > threshold else 0 for p in all_preds]
    acc = accuracy_score(all_labels, preds_bin)
    f1 = f1_score(all_labels, preds_bin)
    try:
        auc = roc_auc_score(all_labels, all_preds)
    except ValueError:
        auc = float('nan')
    print(f"✅ Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")
    return {"acc": acc, "f1": f1, "auc": auc}

# ============== DataLoader 생성 ===============
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
class_sample_count = np.bincount(train_labels)
weights = 1. / class_sample_count
samples_weight = [weights[label] for label in train_labels]
sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)

train_dataset = ECGDataset(train_files)
test_dataset = ECGDataset(test_files)

train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)
test_loader = DataLoader(test_dataset, batch_size=64)

# ============== 모델 훈련 및 결과 출력 ===============
models = [CNN(), LSTMModel()]
model_names = ["CNN", "LSTM"]
results = {}

for model, name in zip(models, model_names):
    print(f"\n📌 [{name}] Training and Evaluation...")
    model_type = 'cnn' if name == "CNN" else 'lstm'
    metrics = train_eval(model, train_loader, test_loader, model_type=model_type)
    if metrics:
        results[name] = metrics
        torch.save(model.state_dict(), f"{name}_best_model.pt")
        print(f"💾 저장됨: {name}_best_model.pt")

# ============== 라벨 분포 출력 ===============
train_labels = [label_dict[f.split(".")[0]] for f in train_files]
print("🔍 학습 라벨 분포:", dict(zip(*np.unique(train_labels, return_counts=True))))

test_labels = [label_dict[f.split(".")[0]] for f in test_files]
print("🔍 테스트 라벨 분포:", dict(zip(*np.unique(test_labels, return_counts=True))))

# ============== 최종 결과 ===============
print("\n📊 최종 결과:", results)

import torch
import numpy as np
import os
import csv
from tqdm import tqdm

# ⚙️ 설정
data_dir = "/content/drive/MyDrive/mimic_ecg_filtered"
model_path = "CNN_best_model.pt"
output_csv_path = "/content/drive/MyDrive/ECG_inference_result.csv"
threshold = 0.3  # 원하는 임계값
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 🧠 CNN 모델 정의 (기존과 동일)
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(12, 16, kernel_size=7, padding=3)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)
        self.pool2 = nn.MaxPool1d(2)
        self.fc1 = nn.Linear(32 * 1250, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = self.pool1(torch.relu(self.conv1(x)))
        x = self.pool2(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# 📥 모델 로드
model = CNN()
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

# 📂 추론 대상 파일 리스트
all_files = [f for f in os.listdir(data_dir) if f.endswith(".npy")]
evaluation_files = sorted(all_files)  # 모든 ECG 파일 대상으로 추론 수행

# 📤 결과 저장
results = []

for filename in tqdm(evaluation_files, desc="🔍 추론 중"):
    filepath = os.path.join(data_dir, filename)
    signal = np.load(filepath)
    signal = np.nan_to_num(signal)
    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-7)
    signal_tensor = torch.tensor(signal.T, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(signal_tensor).squeeze()
        prob = torch.sigmoid(output).item()
        pred = 1 if prob > threshold else 0

    results.append({
        "ecg_id": filename.split(".")[0],
        "PROB": round(prob, 6),
        "PRED": pred
    })

# 💾 CSV로 저장
with open(output_csv_path, "w", newline='') as f:
    writer = csv.DictWriter(f, fieldnames=["ecg_id", "PROB", "PRED"])
    writer.writeheader()
    writer.writerows(results)

print(f"✅ 추론 결과가 저장되었습니다: {output_csv_path}")

import matplotlib.pyplot as plt
import seaborn as sns

def train_eval(model, train_loader, test_loader, model_type='cnn', epochs=5):
    model = model.to(device)
    classes = np.unique(labels_for_split)
    class_weights = compute_class_weight('balanced', classes=classes, y=labels_for_split)
    pos_weight = torch.tensor([class_weights[0]], dtype=torch.float32).to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    loss_history = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for x_cnn, x_lstm, labels in train_loader:
            inputs = x_cnn if model_type == 'cnn' else x_lstm
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print("❌ NaN loss 발생")
                return None
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(train_loader)
        loss_history.append(avg_loss)
        print(f"Epoch {epoch+1} | Loss: {avg_loss:.4f}")

    # 평가 코드는 생략

    return {"loss_history": loss_history}

# --- 훈련 후 loss 시각화 함수 ---

def plot_loss_curve(loss_history, model_name):
    sns.set(style="whitegrid")
    plt.figure(figsize=(8,5))
    plt.plot(range(1, len(loss_history)+1), loss_history, marker='o', color='navy')
    plt.title(f'{model_name} Training Loss Over Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.xticks(range(1, len(loss_history)+1))
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

# --- 예시 사용법 ---
metrics = train_eval(CNN(), train_loader, test_loader, model_type='cnn', epochs=10)
if metrics:
    plot_loss_curve(metrics['loss_history'], "CNN")